{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"AWS Documentation â†’ Chroma Ingestion\n",
        "\n",
        "Interactive notebook for ingesting AWS PDF documentation into a Chroma vector store using Ollama embeddings, batching, metadata enrichment, and guarded timeouts around vector-store writes.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import signal\n",
        "import uuid\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Iterator\n",
        "\n",
        "import httpx\n",
        "import ollama\n",
        "import pandas as pd\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_ollama.embeddings import OllamaEmbeddings\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 46 rows from AWSDocs.csv\n"
          ]
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    data_root: Path = Path(\".\")\n",
        "    csv_filename: str = \"AWSDocs.csv\"\n",
        "    chroma_dirname: str = \"chroma_db_AWSDocs\"\n",
        "    collection_name: str = \"AWSDocs\"\n",
        "    embedding_model: str = \"nomic-embed-text\"\n",
        "    chunk_size: int = 1000\n",
        "    chunk_overlap: int = 200\n",
        "    batch_size: int = 50\n",
        "    add_timeout_seconds: int = 60\n",
        "    request_timeout: httpx.Timeout = field(\n",
        "        default_factory=lambda: httpx.Timeout(30.0, connect=5.0)\n",
        "    )\n",
        "    header_splits: tuple[tuple[str, str], ...] = (\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def csv_path(self) -> Path:\n",
        "        return self.data_root / self.csv_filename\n",
        "\n",
        "    @property\n",
        "    def chroma_path(self) -> Path:\n",
        "        return self.data_root / self.chroma_dirname\n",
        "\n",
        "\n",
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:09:23,790 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
          ]
        }
      ],
      "source": [
        "def load_documents_frame(config: Config) -> pd.DataFrame:\n",
        "    if not config.csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Input CSV not found at {config.csv_path}\")\n",
        "\n",
        "    frame = pd.read_csv(config.csv_path)\n",
        "    print(f\"Loaded {len(frame)} rows from {config.csv_path}\")\n",
        "    return frame\n",
        "\n",
        "\n",
        "df = load_documents_frame(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
        "embeddings._client = ollama.Client(host=embeddings.base_url, timeout=config.request_timeout)\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=config.collection_name,\n",
        "    persist_directory=str(config.chroma_path),\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(list(config.header_splits))\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=config.chunk_size,\n",
        "    chunk_overlap=config.chunk_overlap,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def time_limit(seconds: int, timeout_message: str):\n",
        "    def _raise_timeout(_signum, _frame):\n",
        "        raise TimeoutError(timeout_message)\n",
        "\n",
        "    original_handler = signal.signal(signal.SIGALRM, _raise_timeout)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "        signal.signal(signal.SIGALRM, original_handler)\n",
        "\n",
        "\n",
        "def fetch_markdown(url: str) -> str:\n",
        "    loader = DoclingLoader(\n",
        "        file_path=url,\n",
        "        export_type=ExportType.MARKDOWN,\n",
        "    )\n",
        "    docs_as_markdown = loader.load()\n",
        "    if not docs_as_markdown:\n",
        "        raise ValueError(f\"No content returned for {url}\")\n",
        "    return docs_as_markdown[0].page_content\n",
        "\n",
        "\n",
        "def should_skip_chunk(chunk) -> tuple[bool, str]:\n",
        "    headers_combined = \" \".join(\n",
        "        header.lower()\n",
        "        for header in (\n",
        "            chunk.metadata.get(\"Header 1\", \"\"),\n",
        "            chunk.metadata.get(\"Header 2\", \"\"),\n",
        "            chunk.metadata.get(\"Header 3\", \"\"),\n",
        "        )\n",
        "        if header\n",
        "    )\n",
        "    if \"table of contents\" in headers_combined:\n",
        "        return True, \"Table of Contents\"\n",
        "\n",
        "    content_to_check = chunk.page_content.strip()\n",
        "    if content_to_check and re.fullmatch(r\"[|\\-\\s]+\", content_to_check):\n",
        "        return True, \"Markdown table fragment\"\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "\n",
        "def split_markdown(markdown_content: str, metadata: dict) -> list:\n",
        "    semantic_chunks = markdown_splitter.split_text(markdown_content)\n",
        "    for chunk in semantic_chunks:\n",
        "        chunk.metadata.update(metadata)\n",
        "\n",
        "    final_chunks = text_splitter.split_documents(semantic_chunks)\n",
        "\n",
        "    filtered_chunks = []\n",
        "    for idx, chunk in enumerate(final_chunks, start=1):\n",
        "        should_skip, reason = should_skip_chunk(chunk)\n",
        "        if should_skip:\n",
        "            print(f\"Skipping chunk {idx} ({reason})\")\n",
        "            continue\n",
        "        filtered_chunks.append(chunk)\n",
        "\n",
        "    return filtered_chunks\n",
        "\n",
        "\n",
        "def chunk_batches(chunks: list, size: int) -> Iterator[list]:\n",
        "    for start in range(0, len(chunks), size):\n",
        "        yield chunks[start : start + size]\n",
        "\n",
        "\n",
        "def store_chunks(chunks: list, *, source: str, config: Config) -> int:\n",
        "    if not chunks:\n",
        "        return 0\n",
        "\n",
        "    chunk_ids = [str(uuid.uuid4()) for _ in chunks]\n",
        "    total_batches = (len(chunks) + config.batch_size - 1) // config.batch_size\n",
        "    stored_chunks = 0\n",
        "\n",
        "    for batch_index, batch_docs in enumerate(\n",
        "        chunk_batches(chunks, config.batch_size), start=1\n",
        "    ):\n",
        "        start_idx = (batch_index - 1) * config.batch_size\n",
        "        end_idx = start_idx + len(batch_docs)\n",
        "        batch_ids = chunk_ids[start_idx:end_idx]\n",
        "\n",
        "        print(\n",
        "            f\"  Adding batch {batch_index}/{total_batches} \"\n",
        "            f\"({len(batch_docs)} chunks)...\"\n",
        "        )\n",
        "\n",
        "        timeout_message = (\n",
        "            f\"Timed out adding batch {batch_index}/{total_batches} \"\n",
        "            f\"for {source} after {config.add_timeout_seconds} seconds\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            with time_limit(config.add_timeout_seconds, timeout_message):\n",
        "                vector_store.add_documents(\n",
        "                    documents=batch_docs,\n",
        "                    ids=batch_ids,\n",
        "                )\n",
        "        except TimeoutError as exc:\n",
        "            print(f\"{exc}. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        stored_chunks += len(batch_docs)\n",
        "\n",
        "    return stored_chunks\n",
        "\n",
        "\n",
        "def normalize_str(value: object) -> str:\n",
        "    if value is None:\n",
        "        return \"\"\n",
        "    text = str(value).strip()\n",
        "    if not text or text.lower() == \"nan\":\n",
        "        return \"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def process_row(row: pd.Series, *, url: str, config: Config) -> int:\n",
        "    metadata = {\n",
        "        \"domain\": normalize_str(row.get(\"Domain\")),\n",
        "        \"service\": normalize_str(row.get(\"Service\")),\n",
        "        \"source\": url,\n",
        "    }\n",
        "\n",
        "    markdown_content = fetch_markdown(url)\n",
        "    chunks = split_markdown(markdown_content, metadata)\n",
        "    return store_chunks(chunks, source=url, config=config)\n",
        "\n",
        "\n",
        "def ingest_dataframe(dataframe: pd.DataFrame, *, config: Config) -> int:\n",
        "    total_chunks = 0\n",
        "\n",
        "    for index, row in dataframe.iterrows():\n",
        "        url = normalize_str(row.get(\"PDF_URL\"))\n",
        "        if not url:\n",
        "            print(f\"Row {index}: missing PDF_URL. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {url}...\")\n",
        "        try:\n",
        "            stored = process_row(row, url=url, config=config)\n",
        "        except (httpx.ReadTimeout, httpx.ConnectTimeout, httpx.TimeoutException) as exc:\n",
        "            print(f\"HTTP timeout while processing {url}: {exc}. Skipping.\")\n",
        "            continue\n",
        "        except ValueError as exc:\n",
        "            print(f\"Data issue for {url}: {exc}. Skipping.\")\n",
        "            continue\n",
        "        except Exception as exc:\n",
        "            print(f\"Error processing {url}: {exc}\")\n",
        "            continue\n",
        "\n",
        "        total_chunks += stored\n",
        "        if stored:\n",
        "            print(f\"Completed {url}: stored {stored} chunks.\")\n",
        "        else:\n",
        "            print(f\"Completed {url}: no chunks to store.\")\n",
        "        print()\n",
        "\n",
        "    print(f\"Ingestion complete. Stored {total_chunks} chunks in total.\")\n",
        "    return total_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:09:54,360 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:09:54,620 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:09:54,622 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:09:54,652 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-12 22:09:54,654 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-12 22:09:54,655 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-11-12 22:09:54,661 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-12 22:09:54,666 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-12 22:09:54,666 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2025-11-12 22:09:56,263 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:09:56,270 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:09:59,356 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:10:00,012 - INFO - Processing document ec2-ug.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/lambda/latest/dg/lambda-dg.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:10:55,867 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:10:55,978 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:10:55,979 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:10:55,980 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:10:55,980 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:10:57,044 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:10:57,646 - INFO - Processing document lambda-dg.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/lambda/latest/dg/lambda-dg.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/AmazonECS/latest/developerguide/ecs-dg.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:12:03,483 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:12:03,616 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:12:03,617 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:12:03,618 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:12:03,618 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:12:04,800 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:12:05,402 - INFO - Processing document ecs-dg.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/AmazonECS/latest/developerguide/ecs-dg.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/eks/latest/userguide/eks-ug.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:13:05,406 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:13:05,607 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:13:05,609 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:13:05,610 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:13:05,610 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:13:06,854 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:13:07,466 - INFO - Processing document eks-ug.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/eks/latest/userguide/eks-ug.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/elasticbeanstalk/latest/dg/awseb-dg.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:14:10,230 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:14:10,343 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:14:10,345 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:14:10,352 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:14:10,353 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:14:11,856 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:14:12,513 - INFO - Processing document awseb-dg.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/elasticbeanstalk/latest/dg/awseb-dg.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/batch/latest/userguide/batch_user.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:15:23,007 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:15:23,066 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:15:23,067 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:15:23,068 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:15:23,068 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:15:24,478 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:15:25,086 - INFO - Processing document batch_user.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/batch/latest/userguide/batch_user.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/AmazonS3/latest/userguide/s3-userguide.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:16:27,820 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:16:28,030 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:16:28,031 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:16:28,033 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:16:28,033 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:16:29,710 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:16:30,454 - INFO - Processing document s3-userguide.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timed out processing https://docs.aws.amazon.com/pdfs/AmazonS3/latest/userguide/s3-userguide.pdf after 60 seconds. Skipping.\n",
            "Processing https://docs.aws.amazon.com/pdfs/ebs/latest/userguide/ebs-ug.pdf...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mprocess_dataframe\u001b[39m\u001b[34m(dataframe)\u001b[39m\n\u001b[32m    137\u001b[39m timeout_message = (\n\u001b[32m    138\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimed out processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROCESS_TIMEOUT_SECONDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m time_limit(PROCESS_TIMEOUT_SECONDS, timeout_message):\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     chunks_added = \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks_added:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunks_added\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mprocess_row\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEmpty PDF_URL value\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m metadata = {\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mDomain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mservice\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mService\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: url,\n\u001b[32m    119\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m markdown_content = \u001b[43mfetch_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m chunks = prepare_chunks(markdown_content, metadata)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m add_chunks_to_store(chunks)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mfetch_markdown\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_markdown\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     16\u001b[39m     loader = DoclingLoader(\n\u001b[32m     17\u001b[39m         file_path=url,\n\u001b[32m     18\u001b[39m         export_type=ExportType.MARKDOWN,\n\u001b[32m     19\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     docs_as_markdown = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m docs_as_markdown:\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo content returned for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_docling/loader.py:117\u001b[39m, in \u001b[36mDoclingLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lazy load documents.\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._file_paths:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     conv_res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_converter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     dl_doc = conv_res.document\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._export_type == ExportType.MARKDOWN:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:39\u001b[39m, in \u001b[36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(wrapped)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_function\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:136\u001b[39m, in \u001b[36mValidateCallWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__pydantic_complete__:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m._create_validators()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__return_pydantic_validator__:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__return_pydantic_validator__(res)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling/document_converter.py:237\u001b[39m, in \u001b[36mDocumentConverter.convert\u001b[39m\u001b[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;129m@validate_call\u001b[39m(config=ConfigDict(strict=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     page_range: PageRange = DEFAULT_PAGE_RANGE,\n\u001b[32m    228\u001b[39m ) -> ConversionResult:\n\u001b[32m    229\u001b[39m     all_res = \u001b[38;5;28mself\u001b[39m.convert_all(\n\u001b[32m    230\u001b[39m         source=[source],\n\u001b[32m    231\u001b[39m         raises_on_error=raises_on_error,\n\u001b[32m   (...)\u001b[39m\u001b[32m    235\u001b[39m         page_range=page_range,\n\u001b[32m    236\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_res\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling/document_converter.py:260\u001b[39m, in \u001b[36mDocumentConverter.convert_all\u001b[39m\u001b[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[39m\n\u001b[32m    257\u001b[39m conv_res_iter = \u001b[38;5;28mself\u001b[39m._convert(conv_input, raises_on_error=raises_on_error)\n\u001b[32m    259\u001b[39m had_result = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhad_result\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPARTIAL_SUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling/document_converter.py:310\u001b[39m, in \u001b[36mDocumentConverter._convert\u001b[39m\u001b[34m(self, conv_input, raises_on_error)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert\u001b[39m(\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m, conv_input: _DocumentConversionInput, raises_on_error: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    307\u001b[39m ) -> Iterator[ConversionResult]:\n\u001b[32m    308\u001b[39m     start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunkify\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconv_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_to_options\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdoc_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass format_options\u001b[39;49;00m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_log\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGoing to convert document batch...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocess_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraises_on_error\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling/utils/utils.py:15\u001b[39m, in \u001b[36mchunkify\u001b[39m\u001b[34m(iterator, chunk_size)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iterator, List):\n\u001b[32m     14\u001b[39m     iterator = \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Take the first element from the iterator\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling/datamodel/document.py:275\u001b[39m, in \u001b[36m_DocumentConversionInput.docs\u001b[39m\u001b[34m(self, format_options)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdocs\u001b[39m(\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    271\u001b[39m     format_options: Mapping[InputFormat, \u001b[33m\"\u001b[39m\u001b[33mBaseFormatOption\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    272\u001b[39m ) -> Iterable[InputDocument]:\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.path_or_stream_iterator:\n\u001b[32m    274\u001b[39m         obj = (\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m             \u001b[43mresolve_source_to_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    277\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m item\n\u001b[32m    278\u001b[39m         )\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mformat\u001b[39m = \u001b[38;5;28mself\u001b[39m._guess_format(obj)\n\u001b[32m    280\u001b[39m         backend: Type[AbstractDocumentBackend]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/docling_core/utils/file.py:111\u001b[39m, in \u001b[36mresolve_source_to_stream\u001b[39m\u001b[34m(source, headers)\u001b[39m\n\u001b[32m    108\u001b[39m     res.raise_for_status()\n\u001b[32m    109\u001b[39m     fname = resolve_remote_filename(http_url=http_url, response_headers=res.headers)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     stream = BytesIO(\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m)\n\u001b[32m    112\u001b[39m     doc_stream = DocumentStream(name=fname, stream=stream)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Kick off ingestion when ready.\n",
        "# total_chunks = ingest_dataframe(df, config=config)\n",
        "# print(f\"Stored {total_chunks} chunks in total.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudyIntelVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
