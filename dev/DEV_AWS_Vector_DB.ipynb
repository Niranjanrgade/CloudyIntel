{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fffb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import signal\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import httpx\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d58ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AWSDocs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH = \"./chroma_db_AWSDocs\"\n",
    "EMBEDDING_MODEL_NAME = \"nomic-embed-text\"\n",
    "BATCH_SIZE = 50\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "PROCESS_TIMEOUT_SECONDS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69e21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced5a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = httpx.Timeout(30.0, connect=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings._client = ollama.Client(host=embeddings.base_url, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095f4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 21:36:52,611 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"AWSDocs\",\n",
    "    persist_directory=CHROMA_DB_PATH,\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4135f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def time_limit(seconds: int, timeout_message: str):\n",
    "    def _raise_timeout(_signum, _frame):\n",
    "        raise TimeoutError(timeout_message)\n",
    "\n",
    "    original_handler = signal.signal(signal.SIGALRM, _raise_timeout)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, original_handler)\n",
    "\n",
    "\n",
    "def fetch_markdown(url: str) -> str:\n",
    "    loader = DoclingLoader(\n",
    "        file_path=url,\n",
    "        export_type=ExportType.MARKDOWN,\n",
    "    )\n",
    "    docs_as_markdown = loader.load()\n",
    "    if not docs_as_markdown:\n",
    "        raise ValueError(f\"No content returned for {url}\")\n",
    "    return docs_as_markdown[0].page_content\n",
    "\n",
    "\n",
    "def should_skip_chunk(chunk):\n",
    "    headers_combined = \" \".join(\n",
    "        header.lower()\n",
    "        for header in (\n",
    "            chunk.metadata.get(\"Header 1\", \"\"),\n",
    "            chunk.metadata.get(\"Header 2\", \"\"),\n",
    "            chunk.metadata.get(\"Header 3\", \"\"),\n",
    "        )\n",
    "        if header\n",
    "    )\n",
    "    if \"table of contents\" in headers_combined:\n",
    "        return True, \"Table of Contents\"\n",
    "\n",
    "    content_to_check = chunk.page_content.strip()\n",
    "    if content_to_check and re.fullmatch(r\"[|\\-\\s]+\", content_to_check):\n",
    "        return True, \"Markdown table fragment\"\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def prepare_chunks(markdown_content: str, metadata: dict):\n",
    "    semantic_chunks = markdown_splitter.split_text(markdown_content)\n",
    "    for chunk in semantic_chunks:\n",
    "        chunk.metadata.update(metadata)\n",
    "\n",
    "    final_chunks = text_splitter.split_documents(semantic_chunks)\n",
    "\n",
    "    filtered_chunks = []\n",
    "    for idx, chunk in enumerate(final_chunks, start=1):\n",
    "        should_skip, reason = should_skip_chunk(chunk)\n",
    "        if should_skip:\n",
    "            print(f\"Skipping chunk {idx} ({reason})\")\n",
    "            continue\n",
    "        filtered_chunks.append(chunk)\n",
    "\n",
    "    return filtered_chunks\n",
    "\n",
    "\n",
    "def add_chunks_to_store(chunks):\n",
    "    if not chunks:\n",
    "        return 0\n",
    "\n",
    "    chunk_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "    total_chunks = len(chunks)\n",
    "    total_batches = (total_chunks + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "    for batch_index in range(total_batches):\n",
    "        start = batch_index * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        batch_docs = chunks[start:end]\n",
    "        batch_ids = chunk_ids[start:end]\n",
    "\n",
    "        print(\n",
    "            f\"  Adding batch {batch_index + 1}/{total_batches} \"\n",
    "            f\"({len(batch_docs)} chunks)...\"\n",
    "        )\n",
    "\n",
    "        vector_store.add_documents(\n",
    "            documents=batch_docs,\n",
    "            ids=batch_ids,\n",
    "        )\n",
    "\n",
    "    return total_chunks\n",
    "\n",
    "\n",
    "def process_row(row: pd.Series) -> int:\n",
    "    raw_url = row.get(\"PDF_URL\")\n",
    "\n",
    "    is_missing = False\n",
    "    if raw_url is None:\n",
    "        is_missing = True\n",
    "    else:\n",
    "        try:\n",
    "            is_missing = bool(pd.isna(raw_url))\n",
    "        except TypeError:\n",
    "            is_missing = False\n",
    "\n",
    "    if is_missing:\n",
    "        raise ValueError(\"Missing PDF_URL value\")\n",
    "\n",
    "    url = str(raw_url).strip()\n",
    "    if not url or url.lower() == \"nan\":\n",
    "        raise ValueError(\"Empty PDF_URL value\")\n",
    "\n",
    "    metadata = {\n",
    "        \"domain\": row.get(\"Domain\", \"\"),\n",
    "        \"service\": row.get(\"Service\", \"\"),\n",
    "        \"source\": url,\n",
    "    }\n",
    "\n",
    "    markdown_content = fetch_markdown(url)\n",
    "    chunks = prepare_chunks(markdown_content, metadata)\n",
    "    return add_chunks_to_store(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43238e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    raw_url = row.get(\"PDF_URL\")\n",
    "    url = \"\" if raw_url is None else str(raw_url).strip()\n",
    "\n",
    "    if not url or url.lower() == \"nan\":\n",
    "        print(f\"Row {index}: missing PDF_URL. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {url}...\")\n",
    "    try:\n",
    "        timeout_message = (\n",
    "            f\"Timed out processing {url} after {PROCESS_TIMEOUT_SECONDS} seconds\"\n",
    "        )\n",
    "        with time_limit(PROCESS_TIMEOUT_SECONDS, timeout_message):\n",
    "            chunks_added = process_row(row)\n",
    "\n",
    "        if chunks_added:\n",
    "            print(f\"Completed {url}: stored {chunks_added} chunks.\")\n",
    "        else:\n",
    "            print(f\"Completed {url}: no chunks to store.\")\n",
    "\n",
    "    except TimeoutError as exc:\n",
    "        print(f\"{exc}. Skipping.\")\n",
    "        continue\n",
    "    except (httpx.ReadTimeout, httpx.ConnectTimeout, httpx.TimeoutException) as exc:\n",
    "        print(f\"HTTP timeout while processing {url}: {exc}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as exc:\n",
    "        print(f\"Error processing {url}: {exc}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ac368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 21:37:07,548 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-12 21:37:07,831 - INFO - Going to convert document batch...\n",
      "2025-11-12 21:37:07,832 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-12 21:37:07,850 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-12 21:37:07,852 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-12 21:37:07,852 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-12 21:37:07,858 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-12 21:37:07,865 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-12 21:37:07,865 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-12 21:37:09,124 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-12 21:37:09,129 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 21:37:10,817 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 21:37:11,797 - INFO - Processing document ec2-ug.pdf\n",
      "2025-11-12 21:54:55,650 - INFO - Finished converting document ec2-ug.pdf in 1071.71 sec.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24fcf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'Header 2': 'How standard burstable performance instances work'}, page_content='When a burstable performance instance configured as standard is in a running state, it continuously earns (at a millisecond-level resolution) a set rate of earned credits per hour. For T2 Standard, when the instance is stopped, it loses all its accrued credits, and its credit balance is reset to zero. When it is restarted, it receives a new set of launch credits, and begins to accrue earned credits. For T4g, T3a, and T3 Standard instances, the CPU credit balance persists for seven days after the instance stops and the credits are lost thereafter. If you start the instance within seven days, no credits are lost.'),\n",
       "  Document(metadata={'Header 2': 'How standard burstable performance instances work'}, page_content='T2 Standard instances receive two types of CPU credits: earned credits and launch credits . When a T2 Standard instance is in a running state, it continuously earns (at a millisecond-level resolution) a set rate of earned credits per hour. At start, it has not yet earned credits for a good startup experience; therefore, to provide a good startup experience, it receives launch credits at start, which it spends first while it accrues earned credits.  \\nT4g, T3a, and T3 instances do not receive launch credits because they support Unlimited mode. The Unlimited mode credit configuration enables T4g, T3a, and T3 instances to use as much CPU as needed to burst beyond baseline and for as long as needed.')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8327cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82960295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb2670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8c403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0124fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53e361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ac51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudyIntelVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
