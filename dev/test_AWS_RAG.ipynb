{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9d4a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "092849cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AWSDocs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e419fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = df.iloc[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9cf034c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain                                             Analytics\n",
       "Service                                            sagemaker\n",
       "PDF_URL    https://docs.aws.amazon.com/pdfs/next-generati...\n",
       "Name: 43, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9af2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingLoader(\n",
    "    file_path=first_row['PDF_URL'],\n",
    "    export_type=ExportType.MARKDOWN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "619e1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:53:43,321 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-11 18:53:43,436 - INFO - Going to convert document batch...\n",
      "2025-11-11 18:53:43,446 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-11 18:53:43,462 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-11 18:53:43,469 - INFO - Accelerator device: 'mps'\n",
      "2025-11-11 18:53:49,093 - INFO - Accelerator device: 'mps'\n",
      "2025-11-11 18:53:50,343 - INFO - Processing document next-generation-sagemaker-ug.pdf\n",
      "2025-11-11 18:55:25,649 - INFO - Finished converting document next-generation-sagemaker-ug.pdf in 102.75 sec.\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a80a5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e358f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd5859b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 121 chunks based on Markdown headers.\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\")\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "splits = markdown_splitter.split_text(markdown_content)\n",
    "\n",
    "# 'splits' is now a list of Document objects split by headers\n",
    "print(f\"Split document into {len(splits)} chunks based on Markdown headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "48feda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50  \n",
    "CHUNK_SIZE = 2000 # Safety net: Max characters per *final* chunk\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6cf1390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01ce3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH = \"./chroma_db_AWS_Docs\"\n",
    "EMBEDDING_MODEL_NAME = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0ab24c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d816d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:56:50,363 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"AWS_Docs\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=CHROMA_DB_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c0657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_list(input_list, batch_size):\n",
    "    \"\"\"Yield successive n-sized chunks from input_list.\"\"\"\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        yield input_list[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "259572e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split aws into 1 chunks\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m doc_batches:\n\u001b[32m     12\u001b[39m     batch_num += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents to the vector store\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m     time.sleep(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:249\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).add_texts != VectorStore.add_texts:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         ids = [\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# If there's at least one valid ID, we'll assume that IDs\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# should be used.\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "\n",
    "    chunks = text_splitter.split_text(split.page_content)\n",
    "\n",
    "    if chunks:\n",
    "        print(f\"Split {split.page_content} into {len(chunks)} chunks\")\n",
    "\n",
    "        doc_batches = batch_list(chunks, BATCH_SIZE)\n",
    "\n",
    "        batch_num = 0\n",
    "        for batch in doc_batches:\n",
    "            batch_num += 1\n",
    "            vector_store.add_documents(batch)\n",
    "            print(f\"Added {len(batch)} documents to the vector store\")\n",
    "            time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04eb75b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:33:35,799 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 documents to the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:33:40,887 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 documents to the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:33:46,044 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 documents to the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:33:51,352 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "do embedding request: Post \"http://127.0.0.1:56969/embedding\": EOF (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(splits)):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents to the vector store\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     time.sleep(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:258\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    257\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m msg = (\n\u001b[32m    260\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    262\u001b[39m )\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:620\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    624\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/langchain_ollama/embeddings.py:301\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    296\u001b[39m     msg = (\n\u001b[32m    297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOllama client is not initialized. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure Ollama is running and the model is loaded.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    299\u001b[39m     )\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/ollama/_client.py:377\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive, dimensions)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    369\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    370\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    375\u001b[39m   dimensions: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    376\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/ollama/_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/MTech/CloudyIntel/cloudyIntelVenv/lib/python3.12/site-packages/ollama/_client.py:133\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: do embedding request: Post \"http://127.0.0.1:56969/embedding\": EOF (status code: 500)"
     ]
    }
   ],
   "source": [
    "for i in range(len(splits)):\n",
    "    vector_store.add_documents(splits[i:i+1])\n",
    "    print(f\"Added {i+1} documents to the vector store\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_process_pdfs(service_docs):\n",
    "    \"\"\"\n",
    "    Downloads, loads, splits, and embeds all PDFs using Docling,\n",
    "    then saves them to Chroma.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Initialize components\n",
    "    \n",
    "    # Embedding model (Using Ollama)\n",
    "\n",
    "\n",
    "    # Chroma vector store\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_DB_PATH,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    # Initialize the two-stage splitters\n",
    "\n",
    "    \n",
    "    # Safety net splitter\n",
    "\n",
    "    \n",
    "    print(f\"Initialized Chroma DB at {CHROMA_DB_PATH}\")\n",
    "    \n",
    "    # 4. Process each service document\n",
    "    total_services = len(service_docs)\n",
    "    for i, service in enumerate(service_docs):\n",
    "        domain = service['Domain']\n",
    "        service_name = service['Service']\n",
    "        url = service['PDF_URL']\n",
    "        \n",
    "        print(f\"\\n--- Processing {i+1}/{total_services}: {domain} - {service_name} ---\")\n",
    "        print(f\"URL: {url}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # A. Load full PDF as Markdown\n",
    "            print(\"Initializing DoclingLoader to export Markdown...\")\n",
    "\n",
    "            \n",
    "            print(\"Loading and parsing with Docling (this may take a while)...\")\n",
    "            docs_as_markdown = loader.load()\n",
    "            \n",
    "            if not docs_as_markdown:\n",
    "                print(\"Docling returned no content. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            markdown_content = docs_as_markdown[0].page_content\n",
    "            \n",
    "            # B. Stage 1 Split: Semantic Markdown splitting\n",
    "            print(\"Splitting document by Markdown headers...\")\n",
    "            semantic_chunks = markdown_splitter.split_text(markdown_content)\n",
    "            \n",
    "            # C. Add our custom metadata to each semantic chunk\n",
    "            for chunk in semantic_chunks:\n",
    "                chunk.metadata[\"domain\"] = domain\n",
    "                chunk.metadata[\"service\"] = service_name\n",
    "                chunk.metadata[\"source\"] = url\n",
    "\n",
    "            # D. Stage 2 Split: Safety net\n",
    "            print(f\"Applying safety split to {len(semantic_chunks)} semantic chunks...\")\n",
    "            final_chunks = text_splitter.split_documents(semantic_chunks)\n",
    "            \n",
    "            # E. Embed and add final chunks to Chroma\n",
    "            if final_chunks:\n",
    "                print(f\"Found {len(final_chunks)} final chunks to add.\")\n",
    "                \n",
    "                # --- ## MODIFIED SECTION ## ---\n",
    "                \n",
    "                # 1. Create a list of unique IDs\n",
    "                chunk_ids = [str(uuid.uuid4()) for _ in final_chunks]\n",
    "                \n",
    "                # 2. Batch both the documents and the IDs\n",
    "                for i in range(0, len(final_chunks), BATCH_SIZE):\n",
    "                    batch_docs = final_chunks[i:i + BATCH_SIZE]\n",
    "                    batch_ids = chunk_ids[i:i + BATCH_SIZE]\n",
    "                    \n",
    "                    batch_num = (i // BATCH_SIZE) + 1\n",
    "                    total_batches = (len(final_chunks) // BATCH_SIZE) + 1\n",
    "\n",
    "                    print(f\"  Adding batch {batch_num}/{total_batches}...\")\n",
    "                    \n",
    "                    # 3. Pass both documents and ids to the vector store\n",
    "                    vector_store.add_documents(\n",
    "                        documents=batch_docs,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                # --- ## END OF MODIFIED SECTION ## ---\n",
    "                \n",
    "                end_time = time.time()\n",
    "                print(f\"Successfully processed {service_name} in {end_time - start_time:.2f} seconds.\")\n",
    "            else:\n",
    "                print(f\"No text chunks extracted from {service_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Failed to process {service_name} from {url}. Skipping. Details: {e}\")\n",
    "\n",
    "    print(\"\\n--- All documents processed! ---\")\n",
    "    print(f\"Vector database is persistent and saved in '{CHROMA_DB_PATH}'\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7697eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 19:18:50,857 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama connection successful.\n"
     ]
    }
   ],
   "source": [
    "OllamaEmbeddings(model=EMBEDDING_MODEL_NAME).embed_query(\"Initial connection test\")\n",
    "print(\"Ollama connection successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8fc0110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 19:18:52,074 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 19:18:52,078 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama embeddings...\n",
      "Initialized Chroma DB at ./chroma_db_AWSDocs\n",
      "\n",
      "--- Processing 1/1: Analytics - sagemaker ---\n",
      "URL: https://docs.aws.amazon.com/pdfs/next-generation-sagemaker/latest/userguide/next-generation-sagemaker-ug.pdf\n",
      "Initializing DoclingLoader to export Markdown...\n",
      "Loading and parsing with Docling (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 19:18:53,379 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-11 19:18:53,453 - INFO - Going to convert document batch...\n",
      "2025-11-11 19:18:53,460 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-11 19:18:53,473 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-11 19:18:53,478 - INFO - Accelerator device: 'mps'\n",
      "2025-11-11 19:19:03,777 - INFO - Accelerator device: 'mps'\n",
      "2025-11-11 19:19:05,341 - INFO - Processing document next-generation-sagemaker-ug.pdf\n",
      "2025-11-11 19:19:45,321 - INFO - Finished converting document next-generation-sagemaker-ug.pdf in 53.07 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting document by Markdown headers...\n",
      "Applying safety split to 121 semantic chunks...\n",
      "Found 141 final chunks to add.\n",
      "  Adding batch 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 19:19:46,846 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to process sagemaker from https://docs.aws.amazon.com/pdfs/next-generation-sagemaker/latest/userguide/next-generation-sagemaker-ug.pdf. Skipping. Details: do embedding request: Post \"http://127.0.0.1:57664/embedding\": EOF (status code: 500)\n",
      "\n",
      "--- All documents processed! ---\n",
      "Vector database is persistent and saved in './chroma_db_AWSDocs'\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CHROMA_DB_PATH):\n",
    "    print(f\"Database already exists at {CHROMA_DB_PATH}.\")\n",
    "    print(\"To re-build, please delete this directory and run again.\")\n",
    "else:\n",
    "    service_docs = parse_csv_data(CSV_DATA)\n",
    "    load_and_process_pdfs(service_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Checking Ollama connection...\")\n",
    "    try:\n",
    "        OllamaEmbeddings(model=EMBEDDING_MODEL_NAME).embed_query(\"Initial connection test\")\n",
    "        print(\"Ollama connection successful.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error: Could not connect to Ollama.\")\n",
    "        print(\"Please make sure the Ollama application is running and you have run:\")\n",
    "        print(f\"ollama pull {EMBEDDING_MODEL_NAME}\")\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudyIntelVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
