{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'AWS Documentation → Chroma Ingestion (Clean Notebook)\\n\\nStandalone workflow for transforming AWS PDF documentation into a Chroma vector store\\nwith Ollama embeddings, metadata propagation, chunk filtering, and write timeouts.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"AWS Documentation → Chroma Ingestion (Clean Notebook)\n",
        "\n",
        "Standalone workflow for transforming AWS PDF documentation into a Chroma vector store\n",
        "with Ollama embeddings, metadata propagation, chunk filtering, and write timeouts.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import signal\n",
        "import uuid\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Iterator\n",
        "\n",
        "import httpx\n",
        "import ollama\n",
        "import pandas as pd\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain_docling.loader import ExportType\n",
        "from langchain_ollama.embeddings import OllamaEmbeddings\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Config(data_root=PosixPath('.'), csv_filename='AWSDocs.csv', chroma_dirname='chroma_db_AWSDocs', collection_name='AWSDocs', embedding_model='nomic-embed-text', chunk_size=1000, chunk_overlap=200, batch_size=50, add_timeout_seconds=60, request_timeout=Timeout(connect=5.0, read=30.0, write=30.0, pool=30.0), header_splits=(('#', 'Header 1'), ('##', 'Header 2'), ('###', 'Header 3')))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    data_root: Path = Path(\".\")\n",
        "    csv_filename: str = \"AWSDocs.csv\"\n",
        "    chroma_dirname: str = \"chroma_db_AWSDocs\"\n",
        "    collection_name: str = \"AWSDocs\"\n",
        "    embedding_model: str = \"nomic-embed-text\"\n",
        "    chunk_size: int = 1000\n",
        "    chunk_overlap: int = 200\n",
        "    batch_size: int = 50\n",
        "    add_timeout_seconds: int = 60\n",
        "    request_timeout: httpx.Timeout = field(\n",
        "        default_factory=lambda: httpx.Timeout(30.0, connect=5.0)\n",
        "    )\n",
        "    header_splits: tuple[tuple[str, str], ...] = (\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def csv_path(self) -> Path:\n",
        "        return self.data_root / self.csv_filename\n",
        "\n",
        "    @property\n",
        "    def chroma_path(self) -> Path:\n",
        "        return self.data_root / self.chroma_dirname\n",
        "\n",
        "\n",
        "config = Config()\n",
        "config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 46 rows from AWSDocs.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Domain</th>\n",
              "      <th>Service</th>\n",
              "      <th>PDF_URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Compute</td>\n",
              "      <td>ec2</td>\n",
              "      <td>https://docs.aws.amazon.com/pdfs/AWSEC2/latest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Compute</td>\n",
              "      <td>lambda</td>\n",
              "      <td>https://docs.aws.amazon.com/pdfs/lambda/latest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Compute</td>\n",
              "      <td>ecs</td>\n",
              "      <td>https://docs.aws.amazon.com/pdfs/AmazonECS/lat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Domain Service                                            PDF_URL\n",
              "0  Compute     ec2  https://docs.aws.amazon.com/pdfs/AWSEC2/latest...\n",
              "1  Compute  lambda  https://docs.aws.amazon.com/pdfs/lambda/latest...\n",
              "2  Compute     ecs  https://docs.aws.amazon.com/pdfs/AmazonECS/lat..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_documents(config: Config) -> pd.DataFrame:\n",
        "    if not config.csv_path.exists():\n",
        "        raise FileNotFoundError(f\"Input CSV not found at {config.csv_path}\")\n",
        "\n",
        "    frame = pd.read_csv(config.csv_path)\n",
        "    print(f\"Loaded {len(frame)} rows from {config.csv_path}\")\n",
        "    return frame\n",
        "\n",
        "\n",
        "df = load_documents(config)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:29:22,380 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
          ]
        }
      ],
      "source": [
        "embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
        "embeddings._client = ollama.Client(host=embeddings.base_url, timeout=config.request_timeout)\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=config.collection_name,\n",
        "    persist_directory=str(config.chroma_path),\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(list(config.header_splits))\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=config.chunk_size,\n",
        "    chunk_overlap=config.chunk_overlap,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def time_limit(seconds: int, timeout_message: str):\n",
        "    def _raise_timeout(_signum, _frame):\n",
        "        raise TimeoutError(timeout_message)\n",
        "\n",
        "    original_handler = signal.signal(signal.SIGALRM, _raise_timeout)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "        signal.signal(signal.SIGALRM, original_handler)\n",
        "\n",
        "\n",
        "def normalize_str(value: object) -> str:\n",
        "    if value is None:\n",
        "        return \"\"\n",
        "    text = str(value).strip()\n",
        "    if not text or text.lower() == \"nan\":\n",
        "        return \"\"\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_markdown(url: str) -> str:\n",
        "    loader = DoclingLoader(\n",
        "        file_path=url,\n",
        "        export_type=ExportType.MARKDOWN,\n",
        "    )\n",
        "    docs_as_markdown = loader.load()\n",
        "    if not docs_as_markdown:\n",
        "        raise ValueError(f\"No content returned for {url}\")\n",
        "    return docs_as_markdown[0].page_content\n",
        "\n",
        "\n",
        "def should_skip_chunk(chunk) -> tuple[bool, str]:\n",
        "    headers_combined = \" \".join(\n",
        "        header.lower()\n",
        "        for header in (\n",
        "            chunk.metadata.get(\"Header 1\", \"\"),\n",
        "            chunk.metadata.get(\"Header 2\", \"\"),\n",
        "            chunk.metadata.get(\"Header 3\", \"\"),\n",
        "        )\n",
        "        if header\n",
        "    )\n",
        "    if \"table of contents\" in headers_combined:\n",
        "        return True, \"Table of Contents\"\n",
        "\n",
        "    content_to_check = chunk.page_content.strip()\n",
        "    if content_to_check and re.fullmatch(r\"[|\\-\\s]+\", content_to_check):\n",
        "        return True, \"Markdown table fragment\"\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "\n",
        "def split_markdown(markdown_content: str, metadata: dict) -> list:\n",
        "    semantic_chunks = markdown_splitter.split_text(markdown_content)\n",
        "    for chunk in semantic_chunks:\n",
        "        chunk.metadata.update(metadata)\n",
        "\n",
        "    final_chunks = text_splitter.split_documents(semantic_chunks)\n",
        "\n",
        "    filtered_chunks = []\n",
        "    for idx, chunk in enumerate(final_chunks, start=1):\n",
        "        should_skip, reason = should_skip_chunk(chunk)\n",
        "        if should_skip:\n",
        "            print(f\"Skipping chunk {idx} ({reason})\")\n",
        "            continue\n",
        "        filtered_chunks.append(chunk)\n",
        "\n",
        "    return filtered_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_batches(chunks: list, size: int) -> Iterator[list]:\n",
        "    for start in range(0, len(chunks), size):\n",
        "        yield chunks[start : start + size]\n",
        "\n",
        "\n",
        "def store_chunks(chunks: list, *, source: str, config: Config) -> int:\n",
        "    if not chunks:\n",
        "        return 0\n",
        "\n",
        "    chunk_ids = [str(uuid.uuid4()) for _ in chunks]\n",
        "    total_batches = (len(chunks) + config.batch_size - 1) // config.batch_size\n",
        "    stored_chunks = 0\n",
        "\n",
        "    for batch_index, batch_docs in enumerate(\n",
        "        chunk_batches(chunks, config.batch_size), start=1\n",
        "    ):\n",
        "        start_idx = (batch_index - 1) * config.batch_size\n",
        "        end_idx = start_idx + len(batch_docs)\n",
        "        batch_ids = chunk_ids[start_idx:end_idx]\n",
        "\n",
        "        print(\n",
        "            f\"  Adding batch {batch_index}/{total_batches} \"\n",
        "            f\"({len(batch_docs)} chunks)...\"\n",
        "        )\n",
        "\n",
        "        timeout_message = (\n",
        "            f\"Timed out adding batch {batch_index}/{total_batches} \"\n",
        "            f\"for {source} after {config.add_timeout_seconds} seconds\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            with time_limit(config.add_timeout_seconds, timeout_message):\n",
        "                vector_store.add_documents(\n",
        "                    documents=batch_docs,\n",
        "                    ids=batch_ids,\n",
        "                )\n",
        "        except TimeoutError as exc:\n",
        "            print(f\"{exc}. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        stored_chunks += len(batch_docs)\n",
        "\n",
        "    return stored_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(row: pd.Series, *, config: Config) -> int:\n",
        "    url = normalize_str(row.get(\"PDF_URL\"))\n",
        "    if not url:\n",
        "        raise ValueError(\"Missing PDF_URL value\")\n",
        "\n",
        "    metadata = {\n",
        "        \"domain\": normalize_str(row.get(\"Domain\")),\n",
        "        \"service\": normalize_str(row.get(\"Service\")),\n",
        "        \"source\": url,\n",
        "    }\n",
        "\n",
        "    markdown_content = fetch_markdown(url)\n",
        "    chunks = split_markdown(markdown_content, metadata)\n",
        "    return store_chunks(chunks, source=url, config=config)\n",
        "\n",
        "\n",
        "def ingest_dataframe(frame: pd.DataFrame, *, config: Config) -> int:\n",
        "    total_chunks = 0\n",
        "\n",
        "    for index, row in frame.iterrows():\n",
        "        url = normalize_str(row.get(\"PDF_URL\"))\n",
        "        if not url:\n",
        "            print(f\"Row {index}: missing PDF_URL. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {url}...\")\n",
        "        try:\n",
        "            stored = process_row(row, config=config)\n",
        "        except (httpx.ReadTimeout, httpx.ConnectTimeout, httpx.TimeoutException) as exc:\n",
        "            print(f\"HTTP timeout while processing {url}: {exc}. Skipping.\")\n",
        "            continue\n",
        "        except ValueError as exc:\n",
        "            print(f\"Data issue for {url}: {exc}. Skipping.\")\n",
        "            continue\n",
        "        except Exception as exc:\n",
        "            print(f\"Error processing {url}: {exc}\")\n",
        "            continue\n",
        "\n",
        "        total_chunks += stored\n",
        "        if stored:\n",
        "            print(f\"Completed {url}: stored {stored} chunks.\")\n",
        "        else:\n",
        "            print(f\"Completed {url}: no chunks to store.\")\n",
        "        print()\n",
        "\n",
        "    print(f\"Ingestion complete. Stored {total_chunks} chunks in total.\")\n",
        "    return total_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:30:32,651 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-12 22:30:32,923 - INFO - Going to convert document batch...\n",
            "2025-11-12 22:30:32,925 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-12 22:30:32,959 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-12 22:30:32,961 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-12 22:30:32,962 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-11-12 22:30:32,968 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-12 22:30:32,973 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-12 22:30:32,973 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2025-11-12 22:30:34,110 - INFO - Auto OCR model selected ocrmac.\n",
            "2025-11-12 22:30:34,118 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:30:35,949 - INFO - Accelerator device: 'mps'\n",
            "2025-11-12 22:30:36,702 - INFO - Processing document ec2-ug.pdf\n"
          ]
        }
      ],
      "source": [
        "# Run when ready to ingest all rows.\n",
        "total_chunks = ingest_dataframe(df, config=config)\n",
        "print(f\"Stored {total_chunks} chunks in total.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudyIntelVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
