{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWS Docs Processor\n",
        "\n",
        "This notebook processes AWS documentation PDFs by:\n",
        "1. Loading each PDF using DoclingLoader with MARKDOWN export type\n",
        "2. Converting to markdown format (`docs_as_markdown = loader.load()`)\n",
        "3. Saving the processed documents to pickle files\n",
        "\n",
        "**Output:**\n",
        "- Individual service files: `{service_name}_docs.pkl` \n",
        "- Combined file: `all_aws_docs.pkl`\n",
        "- Summary file: `processing_summary.json`\n",
        "\n",
        "All files are saved in the `./aws_docs_processed/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain_docling.loader import ExportType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define your data - AWS Services CSV\n",
        "CSV_DATA = \"\"\"Domain,Service,PDF_URL\n",
        "Compute,ec2,https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf\n",
        "Compute,lambda,https://docs.aws.amazon.com/pdfs/lambda/latest/dg/lambda-dg.pdf\n",
        "Compute,ecs,https://docs.aws.amazon.com/pdfs/AmazonECS/latest/developerguide/ecs-dg.pdf\n",
        "Compute,eks,https://docs.aws.amazon.com/pdfs/eks/latest/userguide/eks-ug.pdf\n",
        "Compute,elastic-beanstalk,https://docs.aws.amazon.com/pdfs/elasticbeanstalk/latest/dg/awseb-dg.pdf\n",
        "Compute,batch,https://docs.aws.amazon.com/pdfs/batch/latest/userguide/batch_user.pdf\n",
        "Storage,s3,https://docs.aws.amazon.com/pdfs/AmazonS3/latest/userguide/s3-userguide.pdf\n",
        "Storage,ebs,https://docs.aws.amazon.com/pdfs/ebs/latest/userguide/ebs-ug.pdf\n",
        "Storage,efs,https://docs.aws.amazon.com/pdfs/efs/latest/ug/efs-ug.pdf\n",
        "Storage,glacier,https://docs.aws.amazon.com/pdfs/amazonglacier/latest/dev/glacier-dg.pdf\n",
        "Networking,vpc,https://docs.aws.amazon.com/pdfs/vpc/latest/userguide/vpc-ug.pdf\n",
        "Networking,route53,https://docs.aws.amazon.com/pdfs/Route53/latest/DeveloperGuide/route53-dg.pdf\n",
        "Networking,cloudfront,https://docs.aws.amazon.com/pdfs/AmazonCloudFront/latest/DeveloperGuide/AmazonCloudFront_DevGuide.pdf\n",
        "Networking,api-gateway,https://docs.aws.amazon.com/pdfs/apigateway/latest/developerguide/apigateway-dg.pdf\n",
        "Networking,elasticloadbalancing,https://docs.aws.amazon.com/pdfs/elasticloadbalancing/latest/userguide/elb-ug.pdf\n",
        "Networking,application-load-balancer,https://docs.aws.amazon.com/pdfs/elasticloadbalancing/latest/application/elb-ag.pdf\n",
        "Networking,network-load-balancer,https://docs.aws.amazon.com/pdfs/elasticloadbalancing/latest/network/elb-ng.pdf\n",
        "Networking,gateway-load-balancer,https://docs.aws.amazon.com/pdfs/elasticloadbalancing/latest/gateway/elb-gateway.pdf\n",
        "Security,iam,https://docs.aws.amazon.com/pdfs/IAM/latest/UserGuide/iam-ug.pdf\n",
        "Security,kms,https://docs.aws.amazon.com/pdfs/kms/latest/developerguide/kms-dg.pdf\n",
        "Security,secrets-manager,https://docs.aws.amazon.com/pdfs/secretsmanager/latest/userguide/secretsmanager-userguide.pdf\n",
        "Security,cognito,https://docs.aws.amazon.com/pdfs/cognito/latest/developerguide/cognito-dg.pdf\n",
        "Security,cloudtrail,https://docs.aws.amazon.com/pdfs/awscloudtrail/latest/userguide/awscloudtrail-ug.pdf\n",
        "Database,rds,https://docs.aws.amazon.com/pdfs/AmazonRDS/latest/UserGuide/rds-ug.pdf\n",
        "Database,dynamodb,https://docs.aws.amazon.com/pdfs/amazondynamodb/latest/developerguide/dynamodb-dg.pdf\n",
        "Database,redshift,https://docs.aws.amazon.com/pdfs/redshift/latest/dg/redshift-dg.pdf\n",
        "Database,elasticache,https://docs.aws.amazon.com/pdfs/AmazonElastiCache/latest/dg/redis-ug.pdf\n",
        "Management,cloudwatch,https://docs.aws.amazon.com/pdfs/AmazonCloudWatch/latest/monitoring/acw-ug.pdf\n",
        "Management,cloudformation,https://docs.aws.amazon.com/pdfs/AWSCloudFormation/latest/UserGuide/cfn-ug.pdf\n",
        "Management,ssm,https://docs.aws.amazon.com/pdfs/systems-manager/latest/userguide/systems-manager-ug.pdf\n",
        "Management,codepipeline,https://docs.aws.amazon.com/pdfs/codepipeline/latest/userguide/codepipeline-user.pdf\n",
        "Management,codebuild,https://docs.aws.amazon.com/pdfs/codebuild/latest/userguide/codebuild-user.pdf\n",
        "Management,codeartifact,https://docs.aws.amazon.com/pdfs/codeartifact/latest/ug/codeartifact-user.pdf\n",
        "ApplicationIntegration,sqs,https://docs.aws.amazon.com/pdfs/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dg.pdf\n",
        "ApplicationIntegration,sns,https://docs.aws.amazon.com/pdfs/sns/latest/dg/sns-dg.pdf\n",
        "ApplicationIntegration,step-functions,https://docs.aws.amazon.com/pdfs/step-functions/latest/dg/step-functions-dg.pdf\n",
        "ApplicationIntegration,eventbridge,https://docs.aws.amazon.com/pdfs/eventbridge/latest/userguide/user-guide.pdf\n",
        "Analytics,quicksight,https://docs.aws.amazon.com/pdfs/quicksuite/latest/userguide/amazon-quicksuite-user.pdf\n",
        "Analytics,athena,https://docs.aws.amazon.com/pdfs/athena/latest/ug/athena-ug.pdf\n",
        "Analytics,glue,https://docs.aws.amazon.com/pdfs/glue/latest/dg/glue-dg.pdf\n",
        "Analytics,emr,https://docs.aws.amazon.com/pdfs/emr/latest/ManagementGuide/emr-mgmt.pdf\n",
        "Analytics,kinesis,https://docs.aws.amazon.com/pdfs/streams/latest/dev/kinesis-dg.pdf\n",
        "Analytics,opensearch,https://docs.aws.amazon.com/pdfs/opensearch-service/latest/developerguide/opensearch-service-dg.pdf\n",
        "Analytics,sagemaker,https://docs.aws.amazon.com/pdfs/next-generation-sagemaker/latest/userguide/next-generation-sagemaker-ug.pdf\n",
        "Analytics,lakeformation,https://docs.aws.amazon.com/pdfs/lake-formation/latest/dg/lake-formation-dg.pdf\n",
        "Analytics,datapipeline,https://docs.aws.amazon.com/pdfs/datapipeline/latest/DeveloperGuide/datapipeline-dg.pdf\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Define output directory for saved documents\n",
        "OUTPUT_DIR = \"./aws_docs_processed\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_csv_data(csv_data):\n",
        "    \"\"\"Parses the in-memory CSV string into a list of dictionaries.\"\"\"\n",
        "    service_docs = []\n",
        "    f = io.StringIO(csv_data)\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        service_docs.append(row)\n",
        "    return service_docs\n",
        "\n",
        "# Parse the CSV data\n",
        "service_docs = parse_csv_data(CSV_DATA)\n",
        "print(f\"‚úÖ Found {len(service_docs)} AWS services to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Process each AWS service document\n",
        "# This will load each PDF, convert to markdown, and save the results\n",
        "\n",
        "all_processed_docs = {}  # Dictionary to store all processed documents\n",
        "failed_services = []  # Track services that failed to process\n",
        "\n",
        "total_services = len(service_docs)\n",
        "\n",
        "for i, service in enumerate(service_docs):\n",
        "    domain = service['Domain']\n",
        "    service_name = service['Service']\n",
        "    pdf_url = service['PDF_URL']\n",
        "    \n",
        "    print(f\"\\n--- Processing {i+1}/{total_services}: {domain} - {service_name} ---\")\n",
        "    print(f\"URL: {pdf_url}\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Initialize DoclingLoader with MARKDOWN export type\n",
        "        print(\"Initializing DoclingLoader...\")\n",
        "        loader = DoclingLoader(\n",
        "            file_path=pdf_url,\n",
        "            export_type=ExportType.MARKDOWN,\n",
        "        )\n",
        "        \n",
        "        # Load and convert PDF to markdown\n",
        "        print(\"Loading document (this may take a while)...\")\n",
        "        docs_as_markdown = loader.load()\n",
        "        \n",
        "        if not docs_as_markdown:\n",
        "            print(f\"‚ö†Ô∏è No content extracted from {service_name}. Skipping.\")\n",
        "            failed_services.append(service_name)\n",
        "            continue\n",
        "        \n",
        "        # Store the processed documents\n",
        "        all_processed_docs[service_name] = {\n",
        "            'domain': domain,\n",
        "            'service': service_name,\n",
        "            'url': pdf_url,\n",
        "            'docs': docs_as_markdown,\n",
        "            'num_docs': len(docs_as_markdown)\n",
        "        }\n",
        "        \n",
        "        # Save individual service document\n",
        "        service_output_file = os.path.join(OUTPUT_DIR, f\"{service_name}_docs.pkl\")\n",
        "        with open(service_output_file, 'wb') as f:\n",
        "            pickle.dump(docs_as_markdown, f)\n",
        "        print(f\"üíæ Saved {service_name} to {service_output_file}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        print(f\"‚úÖ Successfully processed {service_name} in {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)\")\n",
        "        print(f\"   Extracted {len(docs_as_markdown)} document(s)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {service_name}: {e}\")\n",
        "        failed_services.append(service_name)\n",
        "        continue\n",
        "\n",
        "print(f\"\\n--- Processing Complete ---\")\n",
        "print(f\"‚úÖ Successfully processed: {len(all_processed_docs)}/{total_services} services\")\n",
        "if failed_services:\n",
        "    print(f\"‚ùå Failed services: {failed_services}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Save all processed documents to a single file\n",
        "all_docs_output_file = os.path.join(OUTPUT_DIR, \"all_aws_docs.pkl\")\n",
        "with open(all_docs_output_file, 'wb') as f:\n",
        "    pickle.dump(all_processed_docs, f)\n",
        "\n",
        "print(f\"üíæ Saved all processed documents to {all_docs_output_file}\")\n",
        "print(f\"   Total services processed: {len(all_processed_docs)}\")\n",
        "\n",
        "# Also save a summary/metadata file\n",
        "summary = {\n",
        "    'total_services': len(all_processed_docs),\n",
        "    'failed_services': failed_services,\n",
        "    'services': {name: {\n",
        "        'domain': info['domain'],\n",
        "        'service': info['service'],\n",
        "        'url': info['url'],\n",
        "        'num_docs': info['num_docs']\n",
        "    } for name, info in all_processed_docs.items()}\n",
        "}\n",
        "\n",
        "import json\n",
        "summary_file = os.path.join(OUTPUT_DIR, \"processing_summary.json\")\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Saved processing summary to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Load Saved Documents\n",
        "\n",
        "To load the saved documents in another notebook, use:\n",
        "\n",
        "```python\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load a specific service\n",
        "service_name = \"ec2\"\n",
        "with open(f\"./aws_docs_processed/{service_name}_docs.pkl\", 'rb') as f:\n",
        "    docs_as_markdown = pickle.load(f)\n",
        "\n",
        "# Or load all documents\n",
        "with open(\"./aws_docs_processed/all_aws_docs.pkl\", 'rb') as f:\n",
        "    all_aws_docs = pickle.load(f)\n",
        "\n",
        "# Access a specific service\n",
        "ec2_docs = all_aws_docs['ec2']['docs']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load saved documents (uncomment to use)\n",
        "# import pickle\n",
        "# \n",
        "# # Load a specific service\n",
        "# service_name = \"ec2\"\n",
        "# with open(f\"./aws_docs_processed/{service_name}_docs.pkl\", 'rb') as f:\n",
        "#     docs_as_markdown = pickle.load(f)\n",
        "# \n",
        "# print(f\"Loaded {len(docs_as_markdown)} documents for {service_name}\")\n",
        "# print(f\"First document preview: {docs_as_markdown[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Verify saved data - Load and check one example\n",
        "if all_processed_docs:\n",
        "    # Get first service as example\n",
        "    first_service = list(all_processed_docs.keys())[0]\n",
        "    example_docs = all_processed_docs[first_service]['docs']\n",
        "    \n",
        "    print(f\"Example: {first_service}\")\n",
        "    print(f\"  Number of documents: {len(example_docs)}\")\n",
        "    if example_docs:\n",
        "        print(f\"  First document type: {type(example_docs[0])}\")\n",
        "        print(f\"  First document metadata keys: {list(example_docs[0].metadata.keys()) if hasattr(example_docs[0], 'metadata') else 'N/A'}\")\n",
        "        print(f\"  First document content preview (first 200 chars):\")\n",
        "        print(f\"  {example_docs[0].page_content[:200]}...\")\n",
        "    \n",
        "    # Verify we can load from pickle file\n",
        "    print(f\"\\n‚úÖ Verification: Loading from saved file...\")\n",
        "    with open(os.path.join(OUTPUT_DIR, f\"{first_service}_docs.pkl\"), 'rb') as f:\n",
        "        loaded_docs = pickle.load(f)\n",
        "    print(f\"   Successfully loaded {len(loaded_docs)} documents from pickle file\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
