{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWS Docs Processor\n",
        "\n",
        "This notebook processes AWS documentation PDFs by:\n",
        "1. Loading each PDF using DoclingLoader with MARKDOWN export type\n",
        "2. Converting to markdown format (`docs_as_markdown = loader.load()`)\n",
        "3. Saving the processed documents to pickle files\n",
        "\n",
        "**Output:**\n",
        "- Individual service files: `{service_name}_docs.pkl` \n",
        "- Combined file: `all_aws_docs.pkl`\n",
        "- Summary file: `processing_summary.json`\n",
        "\n",
        "All files are saved in the `./aws_docs_processed/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain_docling.loader import ExportType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define your data - AWS Services CSV\n",
        "CSV_DATA = \"\"\"Domain,Service,PDF_URL\n",
        "Compute,ec2,https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: ./aws_docs_processed\n"
          ]
        }
      ],
      "source": [
        "# 2. Define output directory for saved documents\n",
        "OUTPUT_DIR = \"./aws_docs_processed\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 1 AWS services to process\n"
          ]
        }
      ],
      "source": [
        "def parse_csv_data(csv_data):\n",
        "    \"\"\"Parses the in-memory CSV string into a list of dictionaries.\"\"\"\n",
        "    service_docs = []\n",
        "    f = io.StringIO(csv_data)\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        service_docs.append(row)\n",
        "    return service_docs\n",
        "\n",
        "# Parse the CSV data\n",
        "service_docs = parse_csv_data(CSV_DATA)\n",
        "print(f\"‚úÖ Found {len(service_docs)} AWS services to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing 1/1: Compute - ec2 ---\n",
            "URL: https://docs.aws.amazon.com/pdfs/AWSEC2/latest/UserGuide/ec2-ug.pdf\n",
            "Initializing DoclingLoader...\n",
            "Loading document (this may take a while)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-13 22:23:51,795 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-11-13 22:23:54,599 - INFO - Going to convert document batch...\n",
            "2025-11-13 22:23:54,599 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
            "2025-11-13 22:23:54,753 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-13 22:23:54,763 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-13 22:23:54,765 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-11-13 22:23:54,787 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-11-13 22:23:54,812 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-11-13 22:23:54,813 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2025-11-13 22:23:54,979 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
            "2025-11-13 22:23:54,982 - INFO - easyocr cannot be used because it is not installed.\n",
            "2025-11-13 22:23:55,882 - INFO - Accelerator device: 'cpu'\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:55,924 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,061 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,065 [RapidOCR] torch.py:54: Using C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,434 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,455 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,456 [RapidOCR] torch.py:54: Using C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,609 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,736 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-13 22:23:56,736 [RapidOCR] torch.py:54: Using C:\\Users\\niran.NIRANJAN_GADE\\OneDrive\\Desktop\\Projects\\CloudyIntel\\env_cloudyIntel\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "2025-11-13 22:23:57,253 - INFO - Auto OCR model selected rapidocr with torch.\n",
            "2025-11-13 22:23:57,364 - INFO - Accelerator device: 'cpu'\n",
            "2025-11-13 22:23:59,052 - INFO - Accelerator device: 'cpu'\n",
            "2025-11-13 22:24:00,219 - INFO - Processing document ec2-ug.pdf\n"
          ]
        }
      ],
      "source": [
        "# 3. Process each AWS service document\n",
        "# This will load each PDF, convert to markdown, and save the results\n",
        "\n",
        "all_processed_docs = {}  # Dictionary to store all processed documents\n",
        "failed_services = []  # Track services that failed to process\n",
        "\n",
        "total_services = len(service_docs)\n",
        "\n",
        "for i, service in enumerate(service_docs):\n",
        "    domain = service['Domain']\n",
        "    service_name = service['Service']\n",
        "    pdf_url = service['PDF_URL']\n",
        "    \n",
        "    print(f\"\\n--- Processing {i+1}/{total_services}: {domain} - {service_name} ---\")\n",
        "    print(f\"URL: {pdf_url}\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Initialize DoclingLoader with MARKDOWN export type\n",
        "        print(\"Initializing DoclingLoader...\")\n",
        "        loader = DoclingLoader(\n",
        "            file_path=pdf_url,\n",
        "            export_type=ExportType.MARKDOWN,\n",
        "        )\n",
        "        \n",
        "        # Load and convert PDF to markdown\n",
        "        print(\"Loading document (this may take a while)...\")\n",
        "        docs_as_markdown = loader.load()\n",
        "        \n",
        "        if not docs_as_markdown:\n",
        "            print(f\"‚ö†Ô∏è No content extracted from {service_name}. Skipping.\")\n",
        "            failed_services.append(service_name)\n",
        "            continue\n",
        "        \n",
        "        # Store the processed documents\n",
        "        all_processed_docs[service_name] = {\n",
        "            'domain': domain,\n",
        "            'service': service_name,\n",
        "            'url': pdf_url,\n",
        "            'docs': docs_as_markdown,\n",
        "            'num_docs': len(docs_as_markdown)\n",
        "        }\n",
        "        \n",
        "        # Save individual service document\n",
        "        service_output_file = os.path.join(OUTPUT_DIR, f\"{service_name}_docs.pkl\")\n",
        "        with open(service_output_file, 'wb') as f:\n",
        "            pickle.dump(docs_as_markdown, f)\n",
        "        print(f\"üíæ Saved {service_name} to {service_output_file}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        print(f\"‚úÖ Successfully processed {service_name} in {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)\")\n",
        "        print(f\"   Extracted {len(docs_as_markdown)} document(s)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {service_name}: {e}\")\n",
        "        failed_services.append(service_name)\n",
        "        continue\n",
        "\n",
        "print(f\"\\n--- Processing Complete ---\")\n",
        "print(f\"‚úÖ Successfully processed: {len(all_processed_docs)}/{total_services} services\")\n",
        "if failed_services:\n",
        "    print(f\"‚ùå Failed services: {failed_services}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Save all processed documents to a single file\n",
        "all_docs_output_file = os.path.join(OUTPUT_DIR, \"all_aws_docs.pkl\")\n",
        "with open(all_docs_output_file, 'wb') as f:\n",
        "    pickle.dump(all_processed_docs, f)\n",
        "\n",
        "print(f\"üíæ Saved all processed documents to {all_docs_output_file}\")\n",
        "print(f\"   Total services processed: {len(all_processed_docs)}\")\n",
        "\n",
        "# Also save a summary/metadata file\n",
        "summary = {\n",
        "    'total_services': len(all_processed_docs),\n",
        "    'failed_services': failed_services,\n",
        "    'services': {name: {\n",
        "        'domain': info['domain'],\n",
        "        'service': info['service'],\n",
        "        'url': info['url'],\n",
        "        'num_docs': info['num_docs']\n",
        "    } for name, info in all_processed_docs.items()}\n",
        "}\n",
        "\n",
        "import json\n",
        "summary_file = os.path.join(OUTPUT_DIR, \"processing_summary.json\")\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Saved processing summary to {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Load Saved Documents\n",
        "\n",
        "To load the saved documents in another notebook, use:\n",
        "\n",
        "```python\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load a specific service\n",
        "service_name = \"ec2\"\n",
        "with open(f\"./aws_docs_processed/{service_name}_docs.pkl\", 'rb') as f:\n",
        "    docs_as_markdown = pickle.load(f)\n",
        "\n",
        "# Or load all documents\n",
        "with open(\"./aws_docs_processed/all_aws_docs.pkl\", 'rb') as f:\n",
        "    all_aws_docs = pickle.load(f)\n",
        "\n",
        "# Access a specific service\n",
        "ec2_docs = all_aws_docs['ec2']['docs']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load saved documents (uncomment to use)\n",
        "# import pickle\n",
        "# \n",
        "# # Load a specific service\n",
        "# service_name = \"ec2\"\n",
        "# with open(f\"./aws_docs_processed/{service_name}_docs.pkl\", 'rb') as f:\n",
        "#     docs_as_markdown = pickle.load(f)\n",
        "# \n",
        "# print(f\"Loaded {len(docs_as_markdown)} documents for {service_name}\")\n",
        "# print(f\"First document preview: {docs_as_markdown[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Verify saved data - Load and check one example\n",
        "if all_processed_docs:\n",
        "    # Get first service as example\n",
        "    first_service = list(all_processed_docs.keys())[0]\n",
        "    example_docs = all_processed_docs[first_service]['docs']\n",
        "    \n",
        "    print(f\"Example: {first_service}\")\n",
        "    print(f\"  Number of documents: {len(example_docs)}\")\n",
        "    if example_docs:\n",
        "        print(f\"  First document type: {type(example_docs[0])}\")\n",
        "        print(f\"  First document metadata keys: {list(example_docs[0].metadata.keys()) if hasattr(example_docs[0], 'metadata') else 'N/A'}\")\n",
        "        print(f\"  First document content preview (first 200 chars):\")\n",
        "        print(f\"  {example_docs[0].page_content[:200]}...\")\n",
        "    \n",
        "    # Verify we can load from pickle file\n",
        "    print(f\"\\n‚úÖ Verification: Loading from saved file...\")\n",
        "    with open(os.path.join(OUTPUT_DIR, f\"{first_service}_docs.pkl\"), 'rb') as f:\n",
        "        loaded_docs = pickle.load(f)\n",
        "    print(f\"   Successfully loaded {len(loaded_docs)} documents from pickle file\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_cloudyIntel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
